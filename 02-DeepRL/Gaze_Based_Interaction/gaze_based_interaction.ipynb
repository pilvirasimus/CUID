{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3e0c5f5d",
      "metadata": {
        "id": "3e0c5f5d"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jussippjokinen/CogMod-Tutorial/blob/main/02-DeepRL/Gaze_Based_Interaction/gaze_based_interaction.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fitted-component",
      "metadata": {
        "id": "fitted-component"
      },
      "source": [
        "# A cognitive model of gaze-based interaction\n",
        "\n",
        "Some cognitive models describe behaviour and others predict it. Models that predict behaviour must be capable of generating output without that output being part of the input. In this notebook we demonstrate this property for a model of eye movements. The model is a reinforcement learner that is trained by performing hundreds of thousands of simulated eye movements in search of a target of varying size and distance. The model predicts how many eye movements people will tend to make to find a target of a given size and distance. It predicts inhibition of return. It **predicts** Fitts's Law like behaviour and it predicts that the first eye movement will usually undershoot, rather than overshoot the target.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/03-Reinforcement-Learning/034_Gaze_based_Interaction/image/sub_movements.png\" alt=\"Corati Modeling\" width=\"400\">\n",
        "\n",
        "(source: Meyer, D. E., Abrams, R. A., Kornblum, S., Wright, C. E., & Keith Smith, J. E. (1988). Optimality in human motor performance: ideal control of rapid aimed movements. Psychological review, 95(3), 340.)\n",
        "\n",
        "Note that what might seem the obvious strategy -- aim for where you believe the target is -- is not necessarily the optimal strategy.\n",
        "\n",
        "### The task: Gaze-based interaction\n",
        "\n",
        "Gaze-based interaction is a mode of interaction in which users, including users with a range of movement disabilities, are able to indicate which display item they wish to select by fixating their eyes on it. Confirmation of selection is then made with one of a number of methods, including with a key press, or by holding the fixation for an extended duration of time. The model performs this task for targets with randomly selected location and size.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/03-Reinforcement-Learning/034_Gaze_based_Interaction/image/gaze_task.png\" alt=\"Corati Modeling\" width=\"400\">\n",
        "\n",
        "In the figure, the red lines represent saccades (eye movements). Multiple eye movements are needed to reach the target (the black circle).\n",
        "\n",
        "### Model architecture\n",
        "\n",
        "The model has a simple architecture that you have previously seen in the introduction. The figure is reproduced here:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/howesa/CHI22-CogMod-Tutorial/main/03-Reinforcement-Learning/034_Gaze_based_Interaction/image/cog_arch.png\" alt=\"Corati Modeling\" width=\"700\">\n",
        "\n",
        "- The **control** module makes decisions about where to move the eyes with the oculomotor system. Decisions are conditioned on the current belief about the location of the target.\n",
        "- The **motor** module implements decisions but it is bounded by Gaussian noise, which models noise in the human motor system.\n",
        "- The **environment** models the physics of the world and the task (the location of the target). Given a response from the motor system, a saccade is made to the aim point of the eyes, and a fixation is initiated.\n",
        "- The **perception** module simulates the human capacity to localize a target with foveated vision. The accuracy of the location estimate generated by perception is negatively affected by the eccentricity of the target from the current fixation location.\n",
        "- The **Memory** module stores a representation of the current state. Over the course of an episode a sequence of location estimates will be made. Humans are known to integrate these estimates into a single integrated representation of the location. People are known to do this optimally using a process that can be modelled with Bayesian state estimation. The state estimation constitutes a belief about the location of the target.\n",
        "- The **Utility** module calculates a reward signal given the current belief about the enviornment. The reward signal is used to train the controller.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before proceeding with this notebook you should firrst review the notebooks on foveated vision and on Bayesian integration. These explain how the perception and memory modules work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a427be22",
      "metadata": {
        "id": "a427be22"
      },
      "source": [
        "### Machine learning\n",
        "\n",
        "In order to learn how to perform the task, the model uses implementations of reinforcement learning algorithms in PyTorch known as stable-baselines3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "vL8yMY6q_Rd-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL8yMY6q_Rd-",
        "outputId": "50cedb81-fdf8-4ff8-fd68-f90cbde98917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (1.2.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (2.8.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable_baselines3) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.3)\n",
            "Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable_baselines3\n",
            "Successfully installed stable_baselines3-2.7.0\n"
          ]
        }
      ],
      "source": [
        "# Install stable_baselines3 and the gymnasium environment\n",
        "# This is a well known machine learning library that provides a suite of reinforcement learning methods.\n",
        "# Only needs to be run once\n",
        "\n",
        "!pip install --pre -U stable_baselines3\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "P44tljjb8hBC",
      "metadata": {
        "id": "P44tljjb8hBC",
        "outputId": "9abd680d-7d3f-475b-9d75-4bb04718e47f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-28 11:24:53--  https://raw.githubusercontent.com/jussippjokinen/CogMod-Tutorial/main/02-DeepRL/Gaze_Based_Interaction/gazetools.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5204 (5.1K) [text/plain]\n",
            "Saving to: ‘gazetools.py’\n",
            "\n",
            "\rgazetools.py          0%[                    ]       0  --.-KB/s               \rgazetools.py        100%[===================>]   5.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-28 11:24:53 (21.8 MB/s) - ‘gazetools.py’ saved [5204/5204]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Load local modules\n",
        "# gazetools is a module that contains support functions for modeling gaze-based interaction.\n",
        "# the code below makes use of them but we do not need to understand how they work in this tutorial.\n",
        "\n",
        "!wget https://raw.githubusercontent.com/jussippjokinen/CogMod-Tutorial/main/02-DeepRL/Gaze_Based_Interaction/gazetools.py\n",
        "\n",
        "import gazetools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "obvious-point",
      "metadata": {
        "id": "obvious-point",
        "outputId": "c301b043-33b2-40a0-c6f4-17e97380b4a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# load required standard modules and configure matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "import matplotlib as mpl\n",
        "%matplotlib inline\n",
        "mpl.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "injured-leadership",
      "metadata": {
        "id": "injured-leadership"
      },
      "source": [
        "### Implementation of the Cognitive Architecture as a Python Class\n",
        "\n",
        "The first step to formalise the model architecture presented in the above figure. We do this by specifying a class of cognitive theories and will later define an instance of this class.\n",
        "\n",
        "The class has only a single method, which defines a cycle through the processes defined in the figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "monetary-ivory",
      "metadata": {
        "id": "monetary-ivory"
      },
      "outputs": [],
      "source": [
        "class CognitivePOMDP():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.internal_state = {}\n",
        "\n",
        "    def step(self, ext, action):\n",
        "        ''' Define the cognitive architecture.'''\n",
        "        self._update_state_with_action(action)\n",
        "        response = self._get_response()\n",
        "        external_state, done = ext.external_env(response)\n",
        "        stimulus, stimulus_std = self._get_stimulus(ext.external_state)\n",
        "        self._update_state_with_stimulus(stimulus, stimulus_std)\n",
        "        obs = self._get_obs()\n",
        "        reward = self._get_reward()\n",
        "        return obs, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brazilian-timeline",
      "metadata": {
        "id": "brazilian-timeline"
      },
      "source": [
        "### A theory of gaze-based interaction\n",
        "\n",
        "Each of the entities in the cognitive architecture must be defined so as to make explicit our theory of gaze-based interaction. The theory makes the following assumptions:\n",
        "\n",
        "- The perception of the target location is corrupted by Gaussian noise in human vision.\n",
        "- The standard deviation of noise increases linearly with eccentricity from the fovea.\n",
        "- Sequences of stimuli are noisily perceived and optimally integrated.\n",
        "- Intended eye movements (oculomotor actions) are corrupted by signal dependent Gaussian noise to generate responses.\n",
        "\n",
        "These assumptions are further described in Chen et al. (2021)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accepted-reunion",
      "metadata": {
        "id": "accepted-reunion"
      },
      "outputs": [],
      "source": [
        "class GazeTheory(CognitivePOMDP):\n",
        "\n",
        "    def __init__(self):\n",
        "        ''' Initialise the theoretically motivated parameters.'''\n",
        "        # weight eye movement noise with distance of saccade\n",
        "        self.oculamotor_noise_weight = 0.01 # noise in the movement in itself\n",
        "        # weight noise with eccentricity\n",
        "        self.stimulus_noise_weight = 0.09 # noise based on distance\n",
        "        # step_cost for the reward function\n",
        "        self.step_cost = -1\n",
        "        # super.__init__()\n",
        "\n",
        "    def reset_internal_env(self, external_state):\n",
        "        ''' The internal state includes the fixation location, the latest estimate of\n",
        "        the target location and the target uncertainty. Assumes that there is no\n",
        "        uncertainty in the fixation location.\n",
        "        Assumes that width is known. All numbers are on scale -1 to 1.\n",
        "        The target_std represents the strength of the prior.'''\n",
        "        self.internal_state = {'fixation': np.array([-1,-1]),\n",
        "                               'target': np.array([0,0]),\n",
        "                               'target_std': 0.1,\n",
        "                               'width': external_state['width'],\n",
        "                               'action': np.array([-1,-1])}\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _update_state_with_action(self, action):\n",
        "        self.internal_state['action'] = action\n",
        "\n",
        "    def _get_response(self): # motor\n",
        "        ''' Take an action and add noise.'''\n",
        "        # !!!! should take internal_state as parameter\n",
        "        move_distance = gazetools.get_distance( self.internal_state['fixation'],\n",
        "                                     self.internal_state['action'] )\n",
        "\n",
        "        ocularmotor_noise = np.random.normal(0, self.oculamotor_noise_weight * move_distance,\n",
        "                                        self.internal_state['action'].shape)\n",
        "        # response is action plus noise\n",
        "        response = self.internal_state['action'] + ocularmotor_noise\n",
        "\n",
        "        # update the ocularmotor state (internal)\n",
        "        self.internal_state['fixation'] = response\n",
        "\n",
        "        # make an adjustment if response is out of range.\n",
        "        response = np.clip(response,-1,1)\n",
        "        return response\n",
        "\n",
        "    def _get_stimulus(self, external_state): # perception\n",
        "        ''' define a psychologically plausible stimulus function in which acuity\n",
        "        falls off with eccentricity.'''\n",
        "        eccentricity = gazetools.get_distance( external_state['target'], external_state['fixation'] )\n",
        "        stm_std = self.stimulus_noise_weight * eccentricity\n",
        "        stimulus_noise = np.random.normal(0, stm_std,\n",
        "                                         external_state['target'].shape)\n",
        "        # stimulus is the external target location plus noise\n",
        "        stm = external_state['target'] + stimulus_noise\n",
        "        return stm, stm_std\n",
        "\n",
        "\n",
        "    def _update_state_with_stimulus(self, stimulus, stimulus_std):\n",
        "        posterior, posterior_std = self.bayes_update(stimulus,\n",
        "                                                     stimulus_std,\n",
        "                                                     self.internal_state['target'],\n",
        "                                                     self.internal_state['target_std'])\n",
        "        self.internal_state['target'] = posterior\n",
        "        self.internal_state['target_std'] = posterior_std\n",
        "\n",
        "    def bayes_update(self, stimulus, stimulus_std, belief, belief_std):\n",
        "        ''' A Bayes optimal function that integrates multiple stimului.\n",
        "        The belief is the prior.'''\n",
        "        z1, sigma1 = stimulus, stimulus_std\n",
        "        z2, sigma2 = belief, belief_std\n",
        "        w1 = sigma2**2 / (sigma1**2 + sigma2**2)\n",
        "        w2 = sigma1**2 / (sigma1**2 + sigma2**2)\n",
        "        posterior = w1*z1 + w2*z2\n",
        "        posterior_std = np.sqrt( (sigma1**2 * sigma2**2)/(sigma1**2 + sigma2**2) )\n",
        "        return posterior, posterior_std\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # the Bayesian posterior has already been calculated so just return it.\n",
        "        # also return the target_std so that the controller knows the uncertainty\n",
        "        # of the observation.\n",
        "        #return self.internal_state['target']\n",
        "        return np.array([self.internal_state['target'][0],\n",
        "                        self.internal_state['target'][1],\n",
        "                        self.internal_state['target_std']])\n",
        "\n",
        "    def _get_reward(self):\n",
        "        distance = gazetools.get_distance(self.internal_state['fixation'],\n",
        "                                self.internal_state['target'])\n",
        "\n",
        "        if distance < self.internal_state['width'] / 2: # target reached\n",
        "            reward = 0\n",
        "        else:\n",
        "            reward = -distance # a much better model of the psychological reward function is possible.\n",
        "\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "boolean-missile",
      "metadata": {
        "id": "boolean-missile"
      },
      "source": [
        "### Task environment\n",
        "\n",
        "In order to test the theory we need to define the task environment.\n",
        "\n",
        "The task environment allows the theory to make predictions for a particular task. The theory makes predictions for many more tasks. For example, adaptation to mixed target widths and distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bacterial-archives",
      "metadata": {
        "id": "bacterial-archives"
      },
      "outputs": [],
      "source": [
        "class GazeTask(): # task environment\n",
        "\n",
        "    def __init__(self):\n",
        "        self.target_width = 0.15\n",
        "        self.target_loc_std = 0.3\n",
        "\n",
        "    def reset_external_env(self):\n",
        "        ''' The external_state includes the fixation and target location.\n",
        "        Choose a new target location and reset fixation to the first fixation location.'''\n",
        "\n",
        "        def _get_new_target():\n",
        "            x_target =np.clip(np.random.normal(0, self.target_loc_std),-1,1)\n",
        "            y_target =np.clip(np.random.normal(0, self.target_loc_std),-1,1)\n",
        "            return np.array( [x_target, y_target] )\n",
        "\n",
        "        fx = np.array([-1,-1])\n",
        "        tg = _get_new_target()\n",
        "        self.external_state = {'fixation':fx, 'target':tg, 'width':self.target_width }\n",
        "\n",
        "    def external_env(self, action):\n",
        "        self.external_state['fixation'] = action\n",
        "\n",
        "        # determine when the goal has been achieved.\n",
        "        distance = gazetools.get_distance(self.external_state['fixation'],\n",
        "                                self.external_state['target'])\n",
        "        if distance < self.external_state['width']/2 :\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        return self.external_state, done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "combined-alliance",
      "metadata": {
        "id": "combined-alliance"
      },
      "source": [
        "### Gym (Gymnasium)\n",
        "\n",
        "In order to find an optimal policy we use the theory and external environment to define a machine learning problem, here, making use of the framework defined by one specific library called gym.\n",
        "\n",
        "For further information see: https://gymnasium.farama.org/\n",
        "\n",
        "gym.Env is a class provided by this library. Note that all of the modules of the cognitive architecture are part of gym.env except for the controller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "applicable-sleeve",
      "metadata": {
        "id": "applicable-sleeve"
      },
      "outputs": [],
      "source": [
        "class GazeModel(gym.Env):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        def default_box(x):\n",
        "            return spaces.Box(low=-1, high=1, shape=(x, ), dtype=np.float64)\n",
        "\n",
        "        self.GT = GazeTheory()\n",
        "        self.TX = GazeTask()\n",
        "\n",
        "        # Required by gym. These define the range of each variable.\n",
        "        # Each action has an x,y coordinate therefore the box size is 2.\n",
        "        # Each obs has a an x,y and an uncertainty therefore the box size is 3.\n",
        "        self.action_space = default_box(2)\n",
        "        self.observation_space = default_box(3)\n",
        "\n",
        "        # max_fixations per episode. Used to curtail exploration early in training.\n",
        "        self.max_steps = 500\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        ''' reset the model.'''\n",
        "        self.n_step = 0\n",
        "        self.TX.reset_external_env()\n",
        "        self.GT.reset_internal_env(self.TX.external_state)\n",
        "        obs = self.GT.reset_internal_env( self.TX.external_state )\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        ''' Step through one cycle of the model.'''\n",
        "        obs, reward, done = self.GT.step( self.TX, action )\n",
        "        self.n_step+=1\n",
        "\n",
        "        # give up if been looking for too long\n",
        "        if self.n_step > self.max_steps:\n",
        "            done = True\n",
        "\n",
        "        info = self.get_info()\n",
        "        truncated = False\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def get_info(self):\n",
        "        return {'step': self.n_step,\n",
        "                'target_width': self.TX.target_width,\n",
        "                'target_x': self.TX.external_state['target'][0],\n",
        "                'target_y': self.TX.external_state['target'][1],\n",
        "                'fixate_x':self.TX.external_state['fixation'][0],\n",
        "                'fixate_y':self.TX.external_state['fixation'][1] }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "complex-covering",
      "metadata": {
        "id": "complex-covering"
      },
      "source": [
        "### Test the model\n",
        "\n",
        "Step through the untrained model to check for simple bugs. More comprehensive tests needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ed00048",
      "metadata": {
        "id": "4ed00048"
      },
      "outputs": [],
      "source": [
        "model = GazeModel()\n",
        "\n",
        "model.reset()\n",
        "\n",
        "i=0\n",
        "done = False\n",
        "while not done:\n",
        "    # make a step with a randomly sampled action\n",
        "    obs, reward, done, truncated, info = model.step(model.action_space.sample())\n",
        "    i+=1\n",
        "\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cognitive-stevens",
      "metadata": {
        "id": "cognitive-stevens"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "We can train the model to generate a policy for the controller.\n",
        "\n",
        "By plotting the learning curve we can see whether the performance improves with training and whether the model approaches an optimum performance. We are interested in approximately optimal performance, so if the training curve is not approaching asymptote then we need to train with more timesteps or revise the model.\n",
        "\n",
        "We can see that at first the model uses hundreds of fixations to find the target, this is because it has not yet learned to move the gaze in a way that is informed by the observation. As it learns to do this, it takes fewer steps to gaze at the target and its performance improves.\n",
        "\n",
        "If our problem definition is correct then the model will get more 'human-like' the more that it is trained. In other words, training makes it a better model of interaction.\n",
        "\n",
        "If we assume that people are computationally rational then the optimal solution to a cognitive problem predicts human behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0910ac",
      "metadata": {
        "id": "7d0910ac"
      },
      "outputs": [],
      "source": [
        "timesteps = 100000 # add more\n",
        "\n",
        "controller = gazetools.train(model, timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc728251-2e31-4f0f-ae3d-9f92028a48db",
      "metadata": {
        "id": "dc728251-2e31-4f0f-ae3d-9f92028a48db"
      },
      "outputs": [],
      "source": [
        "gazetools.plot_learning_curve() #if this does not work, it's fine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccba2f2c",
      "metadata": {
        "id": "ccba2f2c"
      },
      "source": [
        "### Increase timesteps\n",
        "\n",
        "100,000 timesteps is not enough to train this model. Try doubling the number of timesteps and train again."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "japanese-georgia",
      "metadata": {
        "id": "japanese-georgia"
      },
      "source": [
        "### Run the model for N trials\n",
        "Run the trained model, save a trace of each episode to csv file, and animate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "after-evans",
      "metadata": {
        "id": "after-evans"
      },
      "outputs": [],
      "source": [
        "data = gazetools.run_model( model, controller, 100, 'behaviour_trace.csv' )\n",
        "\n",
        "gazetools.animate_multiple_episodes(data, n=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c4c96f",
      "metadata": {
        "id": "b6c4c96f"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "- Rerun the model with different parameter settings. Start by trying a different target size. What is the impact on the behaviour?\n",
        "- Discuss in groups how you would use the methods presented in the notebook to build a cognitive model of another task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e77c9c",
      "metadata": {
        "id": "53e77c9c"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "- The cognitive model that we have described above accurately predicts human gaze-based interaction performance (Chen et al., 2021).\n",
        "- It is an example of a **computationally rational** cognitive model. This is because the behaviour is predicted from an approximately optimal policy given hypothesised bounds on cognition.\n",
        "- It should be possible to find an approximately optimal policy using any reinforcement learning algorithm.  The only difference that the algorithm will make is to the efficiency with which solution is found.\n",
        "- The separation of cognitive theory and reinforcement learning algorithm is achieved through the statement of the architecture as what is known as a belief-state Markov Decision Process (a **belief-state MDP**), which is a type of Partially Observable Markov Decision Process (POMDP).\n",
        "- Using reinforcement learning (RL) to model cognition with the approximately optimal policy contrast does not model the human learning process. For work that does use RL to model human learning see Daw and Dayan (2008).\n",
        "- A number of CHI papers have made use of this architecture. See Oulasvirta et al., 2022 for a review.\n",
        "- An important issue concerns how model parameters are fitted to data. See Keuralinen et al., 2023."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "weekly-headquarters",
      "metadata": {
        "id": "weekly-headquarters"
      },
      "source": [
        "### References\n",
        "Chen, X., Acharya, A., Oulasvirta, A., & Howes, A. (2021, May). An adaptive model of gaze-based selection. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-11).\n",
        "\n",
        "Chen, H., Chang, H. J., & Howes, A. (2021, May). Apparently Irrational Choice as Optimal Sequential Decision Making. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 1, pp. 792-800).\n",
        "\n",
        "Dayan, P., & Daw, N. D. (2008). Decision theory, reinforcement learning, and the brain. Cognitive, Affective, & Behavioral Neuroscience, 8(4), 429-453.\n",
        "\n",
        "Oulasvirta, A., Jokinen, J. P., & Howes, A. (2022, April). Computational rationality as a theory of interaction. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (pp. 1-14).\n",
        "\n",
        "Keurulainen, A., Westerlund, I. R., Keurulainen, O., & Howes, A. (2023, April). Amortised Experimental Design and Parameter Estimation for User Models of Pointing. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (pp. 1-17)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}